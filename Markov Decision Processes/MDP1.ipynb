{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mdptoolbox\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "import stolenmdp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build transition matrix\n",
    "transition1=np.zeros((4,12,12))\n",
    "#action0=up\n",
    "#action1=right\n",
    "#action2=down\n",
    "#action3=left\n",
    "s=range(0,12,1)\n",
    "for i in s:\n",
    "    #going up\n",
    "    if i+4<12:\n",
    "        transition1[0,i,i+4]+=1\n",
    "    else:\n",
    "        transition1[0,i,i]+=1\n",
    "    #going right\n",
    "    if i+1 not in [12,8,4]:\n",
    "        transition1[1,i, i+1]+=1\n",
    "    else:\n",
    "        transition1[1,i,i]+=1\n",
    "    #going down\n",
    "    if i-4>-1:\n",
    "        transition1[2,i,i-4]+=1\n",
    "    else:\n",
    "        transition1[2,i,i]+=1\n",
    "    #going left\n",
    "    if i not in [0,4,8]:\n",
    "        transition1[3,i, i-1]+=1\n",
    "    else:\n",
    "        transition1[3,i,i]+=1\n",
    "\n",
    "reward=np.ones((4,12,12))\n",
    "reward1=reward*-1\n",
    "reward1[:,:,10]=100\n",
    "reward1[:,:,6]= -1000\n",
    "reward1[:,:,2]=50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(248.94831398157268, 313.5883139815727, 248.94831398157268, 313.5883139815727, 313.5883139815727, 394.3883139815727, 495.3883139815727, 394.3883139815727, 394.3883139815727, 495.3883139815727, 495.3883139815727, 495.3883139815727)\n",
      "(0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 3)\n",
      "21\n",
      "0.001230001449584961\n"
     ]
    }
   ],
   "source": [
    "#Value Iteration\n",
    "problem1=mdptoolbox.mdp.ValueIteration(transition1, reward1, 0.8)\n",
    "#problem1.setVerbose()\n",
    "problem1.run()\n",
    "value1=problem1.V\n",
    "policy1=problem1.policy\n",
    "print(value1)\n",
    "print(policy1)\n",
    "print(problem1.iter)\n",
    "print(problem1.time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(253.55991772477225, 318.19991772477226, 253.55991772477225, 318.19991772477226, 318.19991772477226, 398.9999177247723, 499.9999177247723, 398.9999177247723, 398.9999177247723, 499.9999177247723, 499.9999177247723, 499.9999177247723)\n",
      "(0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 3)\n",
      "3\n",
      "0.005764961242675781\n"
     ]
    }
   ],
   "source": [
    "#Policy Iteration\n",
    "problem1=mdptoolbox.mdp.PolicyIteration(transition1, reward1, 0.8, eval_type=1)\n",
    "#problem1.setVerbose()\n",
    "problem1.run()\n",
    "value1=problem1.V\n",
    "policy1=problem1.policy\n",
    "print(value1)\n",
    "print(policy1)\n",
    "print(problem1.iter)\n",
    "print(problem1.time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44.400637668762776, 239.49332848294802, 249.9984032986945, 230.43092310487037, 3.3257340905434316, 371.39261435191526, 245.59222144788566, 41.72529759028156, 123.97283205200664, 499.3771916159436, 499.9999628343937, 483.96011449944467)\n",
      "(1, 0, 2, 3, 2, 0, 2, 2, 1, 1, 0, 3)\n",
      "0.2495739459991455\n"
     ]
    }
   ],
   "source": [
    "#Q Learning\n",
    "problem1=mdptoolbox.mdp.QLearning(transition1, reward1, 0.8, n_iter=10000)\n",
    "problem1.setVerbose()\n",
    "time1=time.time()\n",
    "problem1.run()\n",
    "time2=time.time()\n",
    "value1=problem1.V\n",
    "policy1=problem1.policy\n",
    "qtime=time2-time1\n",
    "print(value1)\n",
    "print(policy1)\n",
    "print(qtime)\n",
    "#print(problem1.iter)\n",
    "#print(problem1.time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'stolenmdp' has no attribute 'staceyQLearning'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/staceywies/Documents/CS 7641 Machine Learning/Nothing-to-see-here/Markov Decision Processes/MDP1.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/staceywies/Documents/CS%207641%20Machine%20Learning/Nothing-to-see-here/Markov%20Decision%20Processes/MDP1.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m problem1\u001b[39m=\u001b[39mstolenmdp\u001b[39m.\u001b[39;49mstaceyQLearning(transition1, reward1, \u001b[39m0.8\u001b[39m, n_iter\u001b[39m=\u001b[39m\u001b[39m10000\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/staceywies/Documents/CS%207641%20Machine%20Learning/Nothing-to-see-here/Markov%20Decision%20Processes/MDP1.ipynb#X16sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m problem1\u001b[39m.\u001b[39msetVerbose()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/staceywies/Documents/CS%207641%20Machine%20Learning/Nothing-to-see-here/Markov%20Decision%20Processes/MDP1.ipynb#X16sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m time1\u001b[39m=\u001b[39mtime\u001b[39m.\u001b[39mtime()\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'stolenmdp' has no attribute 'staceyQLearning'"
     ]
    }
   ],
   "source": [
    "problem1=stolenmdp.staceyQLearning(transition1, reward1, 0.8, n_iter=10000)\n",
    "problem1.setVerbose()\n",
    "time1=time.time()\n",
    "problem1.run()\n",
    "time2=time.time()\n",
    "value1=problem1.V\n",
    "policy1=problem1.policy\n",
    "qtime=time2-time1\n",
    "print(value1)\n",
    "print(policy1)\n",
    "print(qtime)\n",
    "#print(problem1.iter)\n",
    "#print(problem1.time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  8.95000012  49.75000045  62.5         49.75092402  -1.23449718\n",
      "  -1.24975791 125.          -1.24999923  18.95        99.75\n",
      " 125.          99.75063469]\n",
      "[1 1 2 3 2 3 0 1 1 1 0 3]\n",
      "[[  -1.224485      8.95000012   -1.17319693   -1.20447851]\n",
      " [  -1.24996311   49.75000045   -1.24999255   -1.22970765]\n",
      " [-975.           -1.24999788   62.5          -1.2453806 ]\n",
      " [  -1.24999893   -1.24998944   -1.2499978    49.75092402]\n",
      " [  -1.23452102 -197.62327059   -1.23449718   -1.24883028]\n",
      " [  -1.2498917  -994.34525246   -1.24989997   -1.24975791]\n",
      " [ 125.           -1.24999979   62.5          -1.24994193]\n",
      " [  -1.24999929   -1.24999923   -1.24999929 -999.84412694]\n",
      " [  -1.17319693   18.95         -1.22459145   -1.17319693]\n",
      " [  -1.25         99.75         -1.24997712   -1.23415856]\n",
      " [ 125.           -1.25       -987.5          -1.24682913]\n",
      " [  -1.25         -1.25         -1.24999972   99.75063469]]\n"
     ]
    }
   ],
   "source": [
    "#Q Learning from scratch\n",
    "q=np.zeros((12,4))\n",
    "reward1,transition1\n",
    "iter=100000\n",
    "eps=0.2\n",
    "gamma=0.2\n",
    "alpha=0.2\n",
    "state=random.randint(0,11)\n",
    "action=random.randint(0,3)\n",
    "nstate=np.argmax(transition1[action,state])\n",
    "prize=reward1[action,state,nstate]\n",
    "qval=q[state,action]\n",
    "maxq=np.max(q[nstate,action])\n",
    "q[state,action]=(1-alpha)*qval+ alpha*(prize+gamma*maxq)\n",
    "i=0\n",
    "while i<iter: #iter\n",
    "    oldstate=state\n",
    "    state=nstate\n",
    "    if random.random()<eps:\n",
    "        action=random.randint(0,3)\n",
    "    else:\n",
    "        action=np.argmax(q[state])\n",
    "    nstate=np.argmax(transition1[action,state])\n",
    "    prize=reward1[action,state,nstate]\n",
    "    qval=q[state,action]\n",
    "    maxq=np.max(q[nstate,action])   \n",
    "    q[state,action]=(1-alpha)*qval+ alpha*(prize+gamma*maxq)\n",
    "\n",
    "    i+=1      \n",
    "\n",
    "valueq=np.amax(q,axis=1)\n",
    "policyq=np.argmax(q,axis=1)\n",
    "print(valueq)\n",
    "print(policyq)\n",
    "print(q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "862f0c301361fa216e65337648d543e9d8ffcab5d13c10cfb69f21da2230b16c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
